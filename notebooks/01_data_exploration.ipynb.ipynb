{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d380fc4-95d1-4151-ab54-35ecc3ef90ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Understanding the Challenge Data\n",
    "\n",
    "### The Mission (kinda cool actually)\n",
    "We're helping young people in Brazil find green jobs. UNICEF partnership, climate action, meaningful careers - the whole deal. \n",
    "And because it's 2025 we will build AI agents that can sift through job descriptions and training programs, match them to people's profiles, and do it efficiently and ethically!\n",
    "\n",
    "**The Brazilian green jobs landscape we're working with:**\n",
    "- **Major cities**: S√£o Paulo (finance & tech hub), Rio de Janeiro (energy & environment), Bras√≠lia (policy & government), Salvador (renewable energy), Recife (innovation centers)\n",
    "- **Key sectors**: Renewable energy (solar, wind, hydro), sustainable agriculture, environmental consulting, green construction, waste management\n",
    "- **Companies leading the charge**: Petrobras (transitioning to renewables), Vale (sustainable mining), Suzano (sustainable forestry), plus hundreds of green startups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375fc5fe-05fa-4aae-b7e2-f3fc627a8f0b",
   "metadata": {},
   "source": [
    "We have a `data` directory with:\n",
    "- **`jobs/`** - 200 job postings\n",
    "- **`trainings/`** - 497 training programs\n",
    "\n",
    "### Quick math reality check\n",
    "697 items √ó however many personas we need to match = potentially expensive if we're not careful with API calls.\n",
    "\n",
    "This is where being smart about it pays off. Literally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec53a31-3ad1-450d-97ae-e6e57fc09ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "Jobs: 200\n",
      "Trainings: 497\n",
      "Total items: 697\n"
     ]
    }
   ],
   "source": [
    "# Let's see what we're working with\n",
    "from pathlib import Path\n",
    "\n",
    "# Count files and get basic statistics\n",
    "jobs_dir = Path('../data/jobs')\n",
    "trainings_dir = Path('../data/trainings')\n",
    "\n",
    "job_files = list(jobs_dir.glob('*.md')) if jobs_dir.exists() else []\n",
    "training_files = list(trainings_dir.glob('*.md')) if trainings_dir.exists() else []\n",
    "\n",
    "print(f\"Dataset Overview:\")\n",
    "print(f\"Jobs: {len(job_files)}\")\n",
    "print(f\"Trainings: {len(training_files)}\")\n",
    "print(f\"Total items: {len(job_files) + len(training_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a173c7f6-7f70-47fd-a31d-1078f06885c6",
   "metadata": {},
   "source": [
    "### Let's look at a job posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600260c2-dd12-48e5-8933-87b28e5f6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to peek at files\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown_file(path: str) -> None:\n",
    "    \"\"\"Display a markdown file in Jupyter - nothing fancy\"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(f\"File not found: {p}\")\n",
    "        return\n",
    "    content = p.read_text(encoding='utf-8', errors='ignore')\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df86ddca-4926-425e-b657-fc731fc5697b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Food Production Supervisor: Help Us Shape the Future of Food Industry\n",
       "\n",
       "**Who We Are Looking For:**\n",
       "We are searching for a talented **Food Production Supervisor** who is passionate about **food manufacturing excellence**. In this role, you won't just be performing tasks; you'll be a key player in **ensuring safe, efficient production operations that meet the highest quality standards**.\n",
       "\n",
       "**Your Impact:**\n",
       "- You will oversee daily production operations, directly affecting product quality and delivery timelines\n",
       "- Your expertise in food safety will protect consumers and maintain regulatory compliance across all processes\n",
       "- You'll optimize production workflows and manage supply chain coordination to reduce waste and improve efficiency\n",
       "- Your leadership will guide production teams toward consistent performance and safety excellence\n",
       "\n",
       "**What You'll Bring:**\n",
       "- A Tecn√≥logo degree with 2 years of hands-on production experience\n",
       "- Solid understanding of sanitation protocols and production line oversight at an intermediate level\n",
       "- Working knowledge of HACCP and GMP food safety standards to maintain compliance\n",
       "- Experience coordinating supply chain logistics to support smooth production flow\n",
       "- Fluency in Portuguese for effective team communication and documentation\n",
       "\n",
       "**Location:** This position is based in S√£o Paulo and requires on-site presence to supervise production operations.\n",
       "\n",
       "**Join Our Mission:**\n",
       "If you're ready to take on a challenge and make a real difference in food production quality and safety, we encourage you to apply."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample job\n",
    "display_markdown_file(job_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a63792-c803-4126-a9c4-b43ce0f9aed0",
   "metadata": {},
   "source": [
    "### And a training program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f42122c-d21b-44e9-a77b-982a7a84030d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Why take this course?**\n",
       "\n",
       "This **Client Support Fundamentals** training will help you:\n",
       "\n",
       "‚úÖ Master essential client interaction techniques at a foundational level  \n",
       "‚úÖ Apply best practices for transparency and compliance  \n",
       "‚úÖ Strengthen your resume with a recognized credential\n",
       "\n",
       "**Course Details:**\n",
       "- **Duration:** 12 weeks\n",
       "- **Format:** Online\n",
       "- **Language:** Portuguese (Brazil)\n",
       "- **Certification:** Yes\n",
       "\n",
       "**Prerequisites:**\n",
       "None\n",
       "\n",
       "**Don't miss the chance to stand out‚Äîregister today!**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display a sample training\n",
    "display_markdown_file(training_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500efaf0-db62-4c18-b85c-f84ce524af55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### What you'll notice\n",
    "\n",
    "Both jobs and trainings have:\n",
    "- **Overview/Description** \n",
    "- **Location** (this matters for matching)\n",
    "- **Prerequisites** (skills, experience levels)\n",
    "- **Outcomes** (for trainings)\n",
    "\n",
    "But here's the kicker: they're not consistently formatted. Some use different headers, different structures, different language. \n",
    "Our solution needs to handle this chaos gracefully. \n",
    "\n",
    "This is why we can't just use regex or simple parsing - we need something smarter: GenAI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845af3e-38d4-4168-b0e4-d99590e5dd5b",
   "metadata": {},
   "source": [
    "### Understanding Tokens - Your Cost Unit\n",
    "\n",
    "LLMs are usually priced via tokens. Usually X$ \"per 1 million tokens\"\n",
    "\n",
    "**Why tokens matter for our challenge:**\n",
    "- **Cost control**: 697 job postings √ó 100 tokens each = 69,700 tokens to process\n",
    "- **Speed**: More tokens = slower responses (matters when processing hundreds of items)  \n",
    "- **Planning**: Models have token limits (128k for all Mistral models)\n",
    "\n",
    "**Quick cost reality check:**\n",
    "- Small model: 69,700 tokens ‚âà $0.007 to classify all jobs\n",
    "- Large model: Same task ‚âà $0.14 (20x more expensive)\n",
    "- For 697 items, choosing the right model matters!\n",
    "\n",
    "**Pro tip**: Always start with the smallest model that can handle your task. You can always upgrade to larger models for complex reasoning later.|\n",
    "\n",
    "### Model Comparison - The Money Talk\n",
    "\n",
    "| Model Name           | Size / Version     | Input Cost (per 1M tokens)  | Output Cost (per 1M tokens)  | Context Window |\n",
    "|----------------------|--------------------|-----------------------------|--------------------------|----------------|\n",
    "| Mistral Large 24-11  | Large              | \\$2.00                       | \\$6.00                        | 128k tokens      |\n",
    "| Mistral Medium 3     | Medium             | \\$0.40                       | \\$2.00                        | 128k tokens      |\n",
    "| Mistral Small 3.1    | Small              | \\$0.10                       | \\$0.30                        | 128k tokens      |\n",
    "\n",
    "**Real talk**: For most filtering/classification tasks, the small model is plenty good and 15x cheaper. Only use the big guns when you really need them.\n",
    "We'll see an example in a minute. But first we need to talk about prompting.\n",
    "\n",
    "### When to use large vs small models?\n",
    "\n",
    "Looking at both responses, they seem pretty similar, right? Both extracted the key information correctly. So why would you ever pay 15x more for the large model?\n",
    "\n",
    "**Small model wins when:**\n",
    "- Simple extraction tasks (skills, location, yes/no questions)\n",
    "- Consistent input format\n",
    "- High-volume processing (like our 697 jobs)\n",
    "- Budget constraints\n",
    "\n",
    "**Large model wins when:**\n",
    "- Complex reasoning required (\"Would this person from Recife be successful in this S√£o Paulo role given the cultural differences?\")\n",
    "- Ambiguous or poorly formatted input\n",
    "- Nuanced analysis (understanding implicit requirements)\n",
    "- Multi-step logical chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75483380-63d7-4b56-944b-403628fc771b",
   "metadata": {},
   "source": [
    "## Using LLMs for Data Filtering\n",
    "\n",
    "### The problem\n",
    "We have 697 items in our dataset. How can we categorize them efficiently without manually reading everything?\n",
    "\n",
    "### Why traditional approaches fail\n",
    "**Regex and keyword matching** would be a nightmare here. Consider these challenges:\n",
    "- Job titles vary: \"Engenheiro de Energia Solar\" vs \"Solar Energy Engineer\" vs \"Renewable Systems Specialist\"  \n",
    "- Skills are described differently: \"2 years experience\" vs \"minimum 24 months\" vs \"experi√™ncia de 2 anos\"\n",
    "- Location formats differ: \"S√£o Paulo, SP\" vs \"Greater S√£o Paulo Area\" vs \"Estado de S√£o Paulo\"\n",
    "- Requirements buried in paragraphs vs structured lists\n",
    "\n",
    "**Rule-based classification** would need hundreds of if-then statements and constant maintenance.\n",
    "\n",
    "### The LLM solution\n",
    "LLMs understand **semantic meaning**, not just keywords:\n",
    "- They recognize \"energia renov√°vel\" and \"renewable energy\" as the same concept\n",
    "- They infer experience levels from contextual clues\n",
    "- They handle inconsistent formatting gracefully  \n",
    "- They can extract implicit information (e.g., senior-level roles often mention \"leadership\")\n",
    "\n",
    "### The trade-offs\n",
    "- **Accuracy**: Much higher than regex, handles edge cases\n",
    "- **Cost**: API calls add up - need to optimize model choice\n",
    "- **Speed**: Slower than regex, but parallel processing helps\n",
    "- **Consistency**: Good with proper prompt design\n",
    "\n",
    "### This design works:\n",
    "\n",
    "- ‚úÖ Constrained outputs: Only 3 possible answers reduces hallucination\n",
    "- ‚úÖ Clear definitions: Explicit criteria for each level\n",
    "- ‚úÖ Simple instruction: 'Just respond with one word' forces compliance\n",
    "- ‚úÖ Context window: 'Analyze whole file' ensures complete understanding\n",
    "- ‚úÖ Large model default: Classification needs reasoning, not just pattern matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb15c8-26cc-49ad-82c5-19c553f4ed01",
   "metadata": {},
   "source": [
    "### Pro tips for production:\n",
    "\n",
    "- Start with large model for accuracy baseline\n",
    "- Test small model on sample - might be sufficient\n",
    "- Use temperature=0 for consistent classifications\n",
    "- Consider few-shot examples for edge cases\n",
    "- Always validate on known examples before scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9b414-bf54-446d-be5f-28eea4390897",
   "metadata": {},
   "source": [
    "### Batch Processing Strategy\n",
    "\n",
    "When you're processing hundreds of items, you need to think about **scale optimization**:\n",
    "\n",
    "**Why batch processing matters:**\n",
    "- **API rate limits**: Most APIs limit requests per minute/hour\n",
    "- **Progress tracking**: Users want to see something happening  \n",
    "- **Error handling**: Individual failures shouldn't kill the whole job\n",
    "- **Memory management**: Don't load all 697 files into memory at once\n",
    "- **Cost monitoring**: Track spending as you go, not at the end\n",
    "\n",
    "**Batch size considerations:**\n",
    "- **Too small** (1-2 items): Lots of overhead, slow overall progress\n",
    "- **Too large** (100+ items): Memory issues, harder to recover from errors\n",
    "- **Sweet spot** (10-25 items): Balance between efficiency and manageability\n",
    "\n",
    "**For our GDSC dataset:**\n",
    "- 697 total items to process\n",
    "- Average ~500 characters per item \n",
    "- At 10 items per batch = 70 batches total\n",
    "- Estimated time: 70 batches √ó 2 seconds = ~2.5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332df58-18fb-4886-8b59-1cccfe5a7e4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises (aka homework)\n",
    "\n",
    "### Exercise 1: Data analysis\n",
    "Build some actual statistics about our dataset:\n",
    "\n",
    "Your mission: analyze the dataset properly\n",
    "What we want to know about Brazilian green jobs:\n",
    "- Geographic distribution (S√£o Paulo, Rio, Bras√≠lia, Salvador, Recife, etc.)\n",
    "- Average token counts per category\n",
    "- Most common skills mentioned\n",
    "- Portuguese vs English content ratio\n",
    "- Green job concentration by region\n",
    "\n",
    "Hint: Use the classify function pattern we just built\n",
    "Consider analyzing:\n",
    "- Job titles: \"Engenheiro Ambiental\" vs \"Environmental Engineer\"\n",
    "- Location patterns: \"S√£o Paulo, SP\" vs \"Greater S√£o Paulo\" vs \"Interior de S√£o Paulo\"\n",
    "- Brazilian-specific skills: Portuguese fluency, local regulations, regional travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04eb90e4-2a10-4a3d-b487-d6b134f13deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Exercise 1: Data analysis\n",
      "Use LLMs to extract domains from job titles and training content\n",
      "Count location mentions across major Brazilian cities\n",
      "Calculate processing costs for different classification approaches\n",
      "Bonus: Identify uniquely Brazilian requirements (e.g., Portuguese fluency, CONAMA compliance)\n",
      "\n",
      "--- Starting Analysis (this may take a few minutes) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing Documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:15<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üìä Dataset Analysis Results ---\n",
      "\n",
      "üìç Geographic Distribution (Top 5):\n",
      "- Unspecified: 8 listings\n",
      "- Recife: 3 listings\n",
      "- Rio de Janeiro: 3 listings\n",
      "- Other: 2 listings\n",
      "- Bras√≠lia: 2 listings\n",
      "\n",
      "üìö Top 5 Job/Training Domains:\n",
      "- Insurance Compliance: 2 listings\n",
      "- Food Production: 1 listings\n",
      "- Visual Production: 1 listings\n",
      "- Food Safety: 1 listings\n",
      "- Procurement Analysis: 1 listings\n",
      "\n",
      "üó£Ô∏è Language Distribution:\n",
      "- Portuguese: 13 listings\n",
      "- Mixed: 7 listings\n",
      "\n",
      "üáßüá∑ Brazilian-Specific Insights:\n",
      "- Listings explicitly requiring Portuguese: 20 (100.0%)\n",
      "- Most common specific terms: [('CLT', 3), ('HACCP', 1), ('GMP', 1)]\n",
      "\n",
      "- Estimated total tokens in dataset: 7,565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()\n",
    "\n",
    "# Make sure we can import the call_llm function\n",
    "# This assumes the notebook is in the 'notebooks' directory\n",
    "# and the function is in 'src/utils/call_llm.py'\n",
    "sys.path.append('..') \n",
    "from src.utils.call_llm import call_llm\n",
    "\n",
    "# Let's assume MISTRAL_API_KEY is set in the environment\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "if not os.getenv(\"MISTRAL_API_KEY\"):\n",
    "    print(\"WARNING: MISTRAL_API_KEY not set. LLM calls will fail.\")\n",
    "    print(\"Please create a .env file in the project root or set the environment variable.\")\n",
    "\n",
    "# --- Data Loading ---\n",
    "jobs_dir = Path('../data/jobs')\n",
    "trainings_dir = Path('../data/trainings')\n",
    "job_files = list(jobs_dir.glob('*.md')) if jobs_dir.exists() else []\n",
    "training_files = list(trainings_dir.glob('*.md')) if trainings_dir.exists() else []\n",
    "all_files = job_files + training_files\n",
    "\n",
    "# --- LLM-based Extractor Function ---\n",
    "def analyze_document(content: str) -> Dict:\n",
    "    \"\"\"Uses an LLM to extract structured data from a job/training document.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following job or training description from Brazil. Extract the specified information in a JSON format.\n",
    "\n",
    "    - \"city\": The primary city. Choose from ['S√£o Paulo', 'Rio de Janeiro', 'Bras√≠lia', 'Salvador', 'Recife', 'Other', 'Unspecified'].\n",
    "    - \"domain\": A short, 2-3 word category (e.g., 'Financial Analysis', 'Solar Energy', 'Environmental Law').\n",
    "    - \"language\": The main language. Choose from ['Portuguese', 'English', 'Mixed'].\n",
    "    - \"requires_portuguese\": A boolean (true/false) if Portuguese fluency is explicitly required.\n",
    "    - \"brazilian_specifics\": A list of any Brazilian-specific regulations or terms mentioned (e.g., 'CONAMA', 'CLT', 'Mata Atl√¢ntica').\n",
    "\n",
    "    Document:\n",
    "    ---\n",
    "    {content}\n",
    "    ---\n",
    "\n",
    "    Respond ONLY with the JSON object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = call_llm(prompt=prompt, model=\"mistral-small-latest\")\n",
    "        # Clean up response in case of markdown formatting\n",
    "        cleaned_response = re.sub(r'```json\\n|\\n```', '', response).strip()\n",
    "        return json.loads(cleaned_response)\n",
    "    except (json.JSONDecodeError, Exception) as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        return {\n",
    "            \"city\": \"Error\", \"domain\": \"Error\", \"language\": \"Error\",\n",
    "            \"requires_portuguese\": False, \"brazilian_specifics\": []\n",
    "        }\n",
    "\n",
    "# --- Main Analysis Logic ---\n",
    "print(\"üìù Exercise 1: Data analysis\")\n",
    "print(\"Use LLMs to extract domains from job titles and training content\")\n",
    "print(\"Count location mentions across major Brazilian cities\")  \n",
    "print(\"Calculate processing costs for different classification approaches\")\n",
    "print(\"Bonus: Identify uniquely Brazilian requirements (e.g., Portuguese fluency, CONAMA compliance)\")\n",
    "print(\"\\n--- Starting Analysis (this may take a few minutes) ---\")\n",
    "\n",
    "analysis_results = []\n",
    "# Using a subset for a quick demonstration. Change to `all_files` to run on the full dataset.\n",
    "files_to_process = all_files[:20] # For quick testing\n",
    "# files_to_process = all_files # For full analysis\n",
    "\n",
    "for file_path in tqdm(files_to_process, desc=\"Analyzing Documents\"):\n",
    "    content = file_path.read_text(encoding='utf-8', errors='ignore')\n",
    "    if content:\n",
    "        analysis_results.append(analyze_document(content))\n",
    "\n",
    "# --- Aggregation and Reporting ---\n",
    "city_counts = Counter(res['city'] for res in analysis_results if 'city' in res)\n",
    "domain_counts = Counter(res['domain'] for res in analysis_results if 'domain' in res)\n",
    "language_counts = Counter(res['language'] for res in analysis_results if 'language' in res)\n",
    "portuguese_req_count = sum(1 for res in analysis_results if res.get('requires_portuguese', False))\n",
    "brazilian_specifics = [spec for res in analysis_results if 'brazilian_specifics' in res for spec in res['brazilian_specifics']]\n",
    "specifics_counts = Counter(spec for spec in brazilian_specifics if spec) # Filter out empty strings\n",
    "\n",
    "total_chars = sum(len(p.read_text(encoding='utf-8', errors='ignore')) for p in files_to_process)\n",
    "# Approximation: 1 token ~ 4 characters\n",
    "total_tokens_approx = total_chars / 4\n",
    "\n",
    "print(\"\\n--- üìä Dataset Analysis Results ---\")\n",
    "print(f\"\\nüìç Geographic Distribution (Top 5):\")\n",
    "for city, count in city_counts.most_common(5):\n",
    "    print(f\"- {city}: {count} listings\")\n",
    "\n",
    "print(f\"\\nüìö Top 5 Job/Training Domains:\")\n",
    "for domain, count in domain_counts.most_common(5):\n",
    "    print(f\"- {domain}: {count} listings\")\n",
    "\n",
    "print(f\"\\nüó£Ô∏è Language Distribution:\")\n",
    "for lang, count in language_counts.items():\n",
    "    print(f\"- {lang}: {count} listings\")\n",
    "\n",
    "print(f\"\\nüáßüá∑ Brazilian-Specific Insights:\")\n",
    "print(f\"- Listings explicitly requiring Portuguese: {portuguese_req_count} ({portuguese_req_count/len(analysis_results):.1%})\")\n",
    "if specifics_counts:\n",
    "    print(f\"- Most common specific terms: {specifics_counts.most_common(3)}\")\n",
    "else:\n",
    "    print(\"- No specific Brazilian terms were commonly found.\")\n",
    "\n",
    "print(f\"\\n- Estimated total tokens in dataset: {total_tokens_approx:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d47e8b-8331-4c26-9f23-afca706dc40f",
   "metadata": {},
   "source": [
    "### Exercise 2: Cost optimization\n",
    "Figure out the cheapest way to process everything:\n",
    "\n",
    "Cost comparison challenge\n",
    "Calculate costs for:\n",
    "1. All 697 items with small model\n",
    "2. All 697 items with large model\n",
    "3. Hybrid: small for classification, large for complex analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd15d294-ca37-4cab-a046-04d93262fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Exercise 2: Cost optimization\n",
      "Which approach gives best quality/cost ratio?\n",
      "What's the break-even point?\n",
      "\n",
      "--- Cost Calculation based on ~7,565 input tokens ---\n",
      "1. All Small Model: $0.0008\n",
      "2. All Large Model: $0.0151\n",
      "3. Hybrid Model (90% small, 10% large): $0.0022\n",
      "\n",
      "--- Analysis ---\n",
      "‚úÖ Best Quality/Cost Ratio: The Hybrid approach is almost always superior.\n",
      "   - It uses the cheap, fast small model for the majority of simple filtering and extraction tasks.\n",
      "   - It saves the expensive, powerful large model for the few items that require complex reasoning, nuance, or summarization.\n",
      "\n",
      "‚ùì Break-even Point: The 'break-even' isn't just about cost, but value.\n",
      "   - You should switch from small to large when the cost of an incorrect classification by the small model is greater than the extra cost of using the large model.\n",
      "   - For this dataset, the large model is 20x more expensive. You only use it when its superior reasoning is absolutely necessary to get a correct result that the small model would fail on.\n"
     ]
    }
   ],
   "source": [
    "# Cost comparison challenge\n",
    "# Calculate costs for:\n",
    "# 1. All 697 items with small model\n",
    "# 2. All 697 items with large model\n",
    "# 3. Hybrid: small for classification, large for complex analysis\n",
    "\n",
    "print(\"üìù Exercise 2: Cost optimization\")\n",
    "print(\"Which approach gives best quality/cost ratio?\")\n",
    "print(\"What's the break-even point?\")\n",
    "\n",
    "# --- Cost Data (from notebook) ---\n",
    "# Prices per 1 Million tokens (Input)\n",
    "COSTS = {\n",
    "    \"small\": 0.10,\n",
    "    \"large\": 2.00\n",
    "}\n",
    "\n",
    "# Total tokens from previous exercise\n",
    "# If exercise 1 was not run, we can re-calculate it here\n",
    "if 'total_tokens_approx' not in locals():\n",
    "    all_files = list(Path('../data/jobs').glob('*.md')) + list(Path('../data/trainings').glob('*.md'))\n",
    "    total_chars = sum(len(p.read_text(encoding='utf-8', errors='ignore')) for p in all_files)\n",
    "    total_tokens_approx = total_chars / 4\n",
    "\n",
    "print(f\"\\n--- Cost Calculation based on ~{total_tokens_approx:,.0f} input tokens ---\")\n",
    "\n",
    "# --- Scenario 1: All Small ---\n",
    "cost_all_small = (total_tokens_approx / 1_000_000) * COSTS[\"small\"]\n",
    "print(f\"1. All Small Model: ${cost_all_small:.4f}\")\n",
    "\n",
    "# --- Scenario 2: All Large ---\n",
    "cost_all_large = (total_tokens_approx / 1_000_000) * COSTS[\"large\"]\n",
    "print(f\"2. All Large Model: ${cost_all_large:.4f}\")\n",
    "\n",
    "# --- Scenario 3: Hybrid Approach ---\n",
    "# Assumption: 90% of tasks (like initial filtering/classification) use the small model.\n",
    "# The remaining 10% of complex items require the large model for deeper analysis.\n",
    "hybrid_ratio_small = 0.90\n",
    "hybrid_ratio_large = 0.10\n",
    "cost_hybrid = ((total_tokens_approx * hybrid_ratio_small / 1_000_000) * COSTS[\"small\"]) + \\\n",
    "              ((total_tokens_approx * hybrid_ratio_large / 1_000_000) * COSTS[\"large\"])\n",
    "print(f\"3. Hybrid Model (90% small, 10% large): ${cost_hybrid:.4f}\")\n",
    "\n",
    "print(\"\\n--- Analysis ---\")\n",
    "print(\"‚úÖ Best Quality/Cost Ratio: The Hybrid approach is almost always superior.\")\n",
    "print(\"   - It uses the cheap, fast small model for the majority of simple filtering and extraction tasks.\")\n",
    "print(\"   - It saves the expensive, powerful large model for the few items that require complex reasoning, nuance, or summarization.\")\n",
    "print(\"\\n‚ùì Break-even Point: The 'break-even' isn't just about cost, but value.\")\n",
    "print(\"   - You should switch from small to large when the cost of an incorrect classification by the small model is greater than the extra cost of using the large model.\")\n",
    "print(f\"   - For this dataset, the large model is {COSTS['large']/COSTS['small']:.0f}x more expensive. You only use it when its superior reasoning is absolutely necessary to get a correct result that the small model would fail on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2105a6b1-8651-417f-a36e-fc95ffe1b8c5",
   "metadata": {},
   "source": [
    "### Exercise 3: Green jobs detector\n",
    "Build a classifier for sustainability-related jobs:\n",
    "\n",
    "```pthon\n",
    "# Green jobs classifier for Brazilian context\n",
    "def is_green_job(content: str) -> bool:\n",
    "    \"\"\"Detect sustainability/climate-related jobs and trainings in Brazilian context\"\"\"\n",
    "    # Your implementation here\n",
    "    # Look for keywords like: \n",
    "    # - English: renewable energy, sustainability, climate, environment, solar, wind\n",
    "    # - Portuguese: energia renov√°vel, sustentabilidade, meio ambiente, solar, e√≥lica\n",
    "    # - Brazilian specifics: CONAMA, Amaz√¥nia, Mata Atl√¢ntica, etanol, biodiesel\n",
    "    # - Companies: Petrobras renewables, Vale sustainability, Suzano forestry\n",
    "    pass\n",
    "```\n",
    "\n",
    "Implement the function and test it...\n",
    "Consider Brazilian green job examples:\n",
    "- Solar panel installer in Northeast Brazil\n",
    "- Environmental consultant for mining companies  \n",
    "- Sustainable agriculture specialist in Cerrado region\n",
    "- Carbon credit analyst for forestry companies\n",
    "- Renewable energy engineer for hydroelectric plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c538ddac-c7bc-4203-87b6-b32b928592e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Exercise 3: Green jobs detector\n",
      "Build a classifier that recognizes sustainability jobs in both Portuguese and English\n",
      "Test it on the dataset - how many green opportunities can you find?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Green Jobs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:06<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Classification Results ---\n",
      "Found 2 green opportunities out of 20 total items (10.0%).\n",
      "\n",
      "--- Bonus Questions ---\n",
      "\n",
      "‚Ä¢ What makes a job 'green' in the Brazilian context?\n",
      "\n",
      "  - Energy Sector: Roles in solar (especially Northeast), wind, hydroelectric power, and biofuels (etanol from sugarcane).\n",
      "  - Natural Resources: Sustainable forestry (like Suzano), responsible mining (Vale's sustainability initiatives), and agribusiness focused on low-carbon practices.\n",
      "  - Conservation: Work related to Brazil's unique biomes like the Amaz√¥nia and Mata Atl√¢ntica.\n",
      "  - Regulation & Policy: Roles requiring knowledge of Brazilian environmental laws like CONAMA regulations.\n",
      "  - Urban Sustainability: Jobs in waste management, green construction, and public policy in major cities.\n",
      "\n",
      "‚Ä¢ How do green job requirements differ between S√£o Paulo (urban) and Amazon region?\n",
      "\n",
      "  - S√£o Paulo: Tends to have more corporate, tech, and finance-focused green jobs. Examples: ESG Analyst, Carbon Credit Trader, Green Tech Startup Developer, Sustainable Finance Manager. These are typically office-based and require business or tech degrees.\n",
      "  - Amazon Region: Jobs are often field-based and hands-on. Examples: Conservation Scientist, Sustainable Forestry Manager, Ecotourism Guide, Environmental Compliance Officer for infrastructure projects. These roles often require degrees in biology, forestry, or environmental science and a willingness to work in remote locations.\n",
      "\n",
      "‚Ä¢ Which green sectors are growing fastest in Brazil?\n",
      "\n",
      "  - Renewable Energy: Solar energy is booming, particularly in the sunny Northeast. Wind power is also a major growth area.\n",
      "  - Biofuels: As a world leader in ethanol, Brazil continues to innovate in second-generation biofuels.\n",
      "  - Carbon Markets & ESG: With increasing global pressure, corporate roles in ESG (Environmental, Social, and Governance) and carbon credit management are rapidly expanding, especially in financial hubs like S√£o Paulo.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Green jobs classifier for Brazilian context\n",
    "def is_green_job(content: str) -> bool:\n",
    "    \"\"\"Detect sustainability/climate-related jobs and trainings in Brazilian context\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert classifier for the Brazilian green economy. A 'green' job or training is one directly involved with environmental sustainability, renewable energy (solar, wind, hydro, biofuels), waste management, conservation, climate action, or sustainable resource management.\n",
    "\n",
    "    Analyze the following text. Consider Brazilian specifics like 'energia renov√°vel', 'sustentabilidade', 'meio ambiente', 'CONAMA', 'Amaz√¥nia', 'etanol', and the sustainability efforts of companies like Petrobras or Suzano.\n",
    "\n",
    "    Based on this, is the following a 'green' opportunity? Respond with only the single word 'yes' or 'no'.\n",
    "\n",
    "    ---\n",
    "    {content}\n",
    "    ---\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use a small model as this is a simple, high-volume classification task\n",
    "        response = call_llm(prompt=prompt, model=\"mistral-small-latest\")\n",
    "        return response.strip().lower() == 'yes'\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during classification: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"üìù Exercise 3: Green jobs detector\")\n",
    "print(\"Build a classifier that recognizes sustainability jobs in both Portuguese and English\")\n",
    "print(\"Test it on the dataset - how many green opportunities can you find?\")\n",
    "\n",
    "# --- Implementation and Testing ---\n",
    "green_jobs_count = 0\n",
    "total_files_processed = 0\n",
    "\n",
    "# Using a subset for a quick demonstration. Change to `all_files` to run on the full dataset.\n",
    "files_to_process = all_files[:20] # For quick testing\n",
    "# files_to_process = all_files # For full analysis\n",
    "\n",
    "for file_path in tqdm(files_to_process, desc=\"Classifying Green Jobs\"):\n",
    "    content = file_path.read_text(encoding='utf-8', errors='ignore')\n",
    "    if content:\n",
    "        if is_green_job(content):\n",
    "            green_jobs_count += 1\n",
    "        total_files_processed += 1\n",
    "\n",
    "print(f\"\\n--- Classification Results ---\")\n",
    "if total_files_processed > 0:\n",
    "    percentage = (green_jobs_count / total_files_processed) * 100\n",
    "    print(f\"Found {green_jobs_count} green opportunities out of {total_files_processed} total items ({percentage:.1f}%).\")\n",
    "else:\n",
    "    print(\"No files were processed.\")\n",
    "\n",
    "print(\"\\n--- Bonus Questions ---\")\n",
    "print(\"\\n‚Ä¢ What makes a job 'green' in the Brazilian context?\")\n",
    "print(\"\"\"\n",
    "  - Energy Sector: Roles in solar (especially Northeast), wind, hydroelectric power, and biofuels (etanol from sugarcane).\n",
    "  - Natural Resources: Sustainable forestry (like Suzano), responsible mining (Vale's sustainability initiatives), and agribusiness focused on low-carbon practices.\n",
    "  - Conservation: Work related to Brazil's unique biomes like the Amaz√¥nia and Mata Atl√¢ntica.\n",
    "  - Regulation & Policy: Roles requiring knowledge of Brazilian environmental laws like CONAMA regulations.\n",
    "  - Urban Sustainability: Jobs in waste management, green construction, and public policy in major cities.\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚Ä¢ How do green job requirements differ between S√£o Paulo (urban) and Amazon region?\")\n",
    "print(\"\"\"\n",
    "  - S√£o Paulo: Tends to have more corporate, tech, and finance-focused green jobs. Examples: ESG Analyst, Carbon Credit Trader, Green Tech Startup Developer, Sustainable Finance Manager. These are typically office-based and require business or tech degrees.\n",
    "  - Amazon Region: Jobs are often field-based and hands-on. Examples: Conservation Scientist, Sustainable Forestry Manager, Ecotourism Guide, Environmental Compliance Officer for infrastructure projects. These roles often require degrees in biology, forestry, or environmental science and a willingness to work in remote locations.\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚Ä¢ Which green sectors are growing fastest in Brazil?\")\n",
    "print(\"\"\"\n",
    "  - Renewable Energy: Solar energy is booming, particularly in the sunny Northeast. Wind power is also a major growth area.\n",
    "  - Biofuels: As a world leader in ethanol, Brazil continues to innovate in second-generation biofuels.\n",
    "  - Carbon Markets & ESG: With increasing global pressure, corporate roles in ESG (Environmental, Social, and Governance) and carbon credit management are rapidly expanding, especially in financial hubs like S√£o Paulo.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a252f9c-4541-48b2-9e60-f041b32b0dc1",
   "metadata": {},
   "source": [
    "## What we learned\n",
    "\n",
    "‚úÖ **Data structure**: 697 items in messy formats  \n",
    "‚úÖ **API basics**: Tokens, models, costs  \n",
    "‚úÖ **Smart filtering**: LLMs > regex for unstructured data  \n",
    "‚úÖ **Cost optimization**: Start small, scale strategically  \n",
    "\n",
    "### The real lessons\n",
    "- Token counting matters when you're processing lots of data\n",
    "- Small models are surprisingly good for classification tasks\n",
    "- Always track costs as you go"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Green Agent)",
   "language": "python",
   "name": "green-agent-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
